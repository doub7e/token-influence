## 2026-02-26 - Qwen3-0.6B-Base GRPO Math Training Scripts (4 GPU)

| Item | Details |
| --- | --- |
| Request | Switch GRPO math training from DeepSeek-R1-Distill-Qwen-1.5B (8 GPU) to Qwen3-0.6B-Base (4 GPU) |
| Delivery | 3 new scripts created; smoke test passed on RunAI; full training job submitted |
| Scope | 4x H100 GPUs, Qwen3-0.6B-Base pretrained model, all hyperparams unchanged |

### Changes
| Path | Change | Why |
| --- | --- | --- |
| `scripts/train/setup_qwen3_0.6b.sh` | Added | Download Qwen3-0.6B-Base model via `download_hf_model.py` |
| `scripts/train/run_archer2.0_qwen3_0.6b_math.sh` | Added | Main training script; 3 params changed vs 1.5B: `exp_name`, `MODEL_PATH`, `n_gpus_per_node=4` |
| `scripts/train/run_archer2.0_qwen3_0.6b_math_smoke_4gpu.sh` | Added | Smoke test wrapper (2 steps, small batches, short sequences) |

### Key Hyperparameters (unchanged from 1.5B config)
| Param | Value |
| --- | --- |
| `train_prompt_bsz` | 64 |
| `n_resp_per_prompt` | 16 |
| `max_response_length` | 32768 |
| `lr` | 1e-6 |
| `ppo_epochs` | 3 |
| `kl_loss_coef` | 0.001 |
| `temperature` | 1.0 |

### Validation
| Check | Evidence | Result |
| --- | --- | --- |
| Smoke test (2 steps, 4 GPU) | RunAI job `archer-qwen3-smoke-0226b` | Pass - both steps completed |
| W&B logging | https://wandb.ai/doub7e/Archer2.0/runs/Archer2.0-Qwen3-0.6B-Math | Pass - metrics logged |
| Memory usage | `max_memory_allocated_gb: 61.6` per GPU | Pass - well within 80GB H100 |
| Grad norm | Step 1: 4.72, Step 2: 3.04 | Pass - healthy range |
| Full training submission | RunAI job `archer-qwen3-math-0226` (4x H100) | Running |

### Git
| Field | Value |
| --- | --- |
| Commit | `4841276` |
| Branch | `main` |
| Remote | `doub7e` |
| Push | No |

### Notes
- 0.6B/4GPU = 150M params/GPU vs 1.5B/8GPU = 187.5M params/GPU, so more memory headroom.
- W&B API key fix was needed (`WANDB_API_KEY` export added to training script from `.codex/secrets/wandb_api_key`).
- First smoke test failed on W&B auth only; all model/FSDP/vLLM infra worked on first attempt.
- Monitor W&B for reward improvement. If `reward/mean` stays at 0 for >100 steps, consider: (1) increase temperature to 1.2, (2) increase `n_resp_per_prompt` to 32, (3) switch to instruct model.
